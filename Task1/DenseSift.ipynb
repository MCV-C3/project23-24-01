{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first read the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_filenames = pickle.load(open('train_images_filenames.dat','rb'))\n",
    "test_images_filenames = pickle.load(open('test_images_filenames.dat','rb'))\n",
    "train_images_filenames = ['..' + n[15:] for n in train_images_filenames]\n",
    "test_images_filenames  = ['..' + n[15:] for n in test_images_filenames]\n",
    "train_labels = pickle.load(open('train_labels.dat','rb')) \n",
    "test_labels = pickle.load(open('test_labels.dat','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the \"../\" from the path to adapt to our structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in range(len(train_images_filenames)):\n",
    "    train_images_filenames[file] = train_images_filenames[file][3:]\n",
    "\n",
    "for file in range(len(test_images_filenames)):\n",
    "    test_images_filenames[file] = test_images_filenames[file][3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell to create KAZE/SIFT/.... object detector and descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector = cv2.SIFT_create(nfeatures=2000)\n",
    "#Detector = cv2.KAZE_create(threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell to use Dense Sift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseSift = True\n",
    "#DenseSift = False\n",
    "keypoint_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the  descriptors for all the train images and subsequently build a numpy array with all the descriptors stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_descriptors = []\n",
    "Train_label_per_descriptor = []\n",
    "\n",
    "for filename,labels in zip(train_images_filenames,train_labels):\n",
    "    ima=cv2.imread(filename)\n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "    if DenseSift:\n",
    "        h, w = gray.shape\n",
    "        step_size_h = int(h/20)\n",
    "        step_size_w = int(w/10)\n",
    "        keypoints = [cv2.KeyPoint(x, y, keypoint_size) for y in range(0, h, step_size_h) for x in range(0, w, step_size_w)]\n",
    "        # Compute SIFT descriptors at each grid point\n",
    "        _, des = Detector.compute(gray, keypoints)\n",
    "    else:\n",
    "        kpt, des=Detector.detectAndCompute(gray,None)\n",
    "    Train_descriptors.append(des)\n",
    "    Train_label_per_descriptor.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6997767857142857\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_images_filenames)\n",
    "n_test = len(test_images_filenames)\n",
    "tradeoff = n_train/(n_train+n_test)\n",
    "print(tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_descriptors = []\n",
    "Test_label_per_descriptor = []\n",
    "\n",
    "for filename,labels in zip(test_images_filenames,test_labels):\n",
    "    ima=cv2.imread(filename)\n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "    kpt,des=Detector.detectAndCompute(gray,None)\n",
    "    Test_descriptors.append(des)\n",
    "    Test_label_per_descriptor.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pkls/test_des_dense_sift\", \"wb\") as file:\n",
    "    pickle.dump(Test_descriptors, file)\n",
    "with open(\"pkls/train_des_dense_sift\", \"wb\") as file:\n",
    "    pickle.dump(Train_descriptors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "for test_label, test_des in zip(Test_label_per_descriptor, Test_descriptors):\n",
    "    best_num_match = 0\n",
    "    best_match_label = None\n",
    "    for train_label, train_des in zip(Train_label_per_descriptor, Train_descriptors):\n",
    "        total_matches = 0\n",
    "        matches = bf.knnMatch(test_des, train_des, k=2)\n",
    "\n",
    "        # Apply ratio test\n",
    "        good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "        total_matches = len(good_matches)\n",
    "\n",
    "        if total_matches > best_num_match:\n",
    "            best_num_match = total_matches\n",
    "            best_match_label = train_label\n",
    "\n",
    "    test_results.append(best_match_label)\n",
    "\n",
    "    with open(\"test_results_dense_sift.pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_corrects = 0\n",
    "for test_label in Test_label_per_descriptor:\n",
    "    num_corrects += 1 if test_label == test_results else 0\n",
    "\n",
    "acc = num_corrects/len(Test_label_per_descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Test_label_per_descriptor))\n",
    "\n",
    "f1_i = 0\n",
    "for classe in classes:\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for test_label, result_label in zip(Test_label_per_descriptor, test_results):\n",
    "        TP += 1 if test_label == classe and result_label == classe else 0\n",
    "        FP += 1 if test_label != classe and result_label == classe else 0\n",
    "        FN += 1 if test_label == classe and result_label != classe else 0\n",
    "\n",
    "    P_i = TP / (TP + FP)\n",
    "    R_i = TP / (TP + FN)\n",
    "    f1_i += 2 * (P_i * R_i) / (P_i + R_i)  \n",
    "    \n",
    "f1 = f1_i / len(classes)\n",
    "print(f\"F1 score = {f1}\")\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
